# -*- coding: utf-8 -*-
"""hyperparameter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y25pNQ-YnjqkGo1vwxFs4sb0e3aEtYC8
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Activation, TimeDistributed, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Bidirectional
from tensorflow.keras.layers import concatenate

from keras.preprocessing import sequence
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix

import os, sys



module_path = '/content/drive/MyDrive/myproject/'
if module_path not in sys.path:
    sys.path.append(module_path)
    from util.dataprep import get_vectors, get_data

import tensorflow as tf
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

# Import necessary packages
from google.colab import drive
import os
import tensorflow as tf
import collections
import sys

# Mount Google Drive to access your files
drive.mount('/content/drive')

# Enable GPU memory growth
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

# CONSTANTS
seed = 42

# Set up the path to your project folder containing util
module_path = '/content/drive/MyDrive/myproject/'
if module_path not in sys.path:
    sys.path.append(module_path)

# Import the get_data function from your dataprep module
from util.dataprep import get_data

# Load your dataset using get_data
X_train, y_train, X_test, y_test, X_val, y_val, vocab_size, embedding_size, vectors = get_data()

# Determine the maximum input dimension
train_max = X_train.max()
test_max = X_test.max()
val_max = X_val.max()
max_all = max([train_max, test_max, val_max])
input_dim = max_all + 1
output_dim = 32

# Check if the data is evenly split
print('Train labels: ', collections.Counter(y_train))
print('Test labels: ', collections.Counter(y_test))
print('Val labels: ', collections.Counter(y_val))

early_stopping = EarlyStopping(monitor='val_loss', patience=4)

def plot_history(history_arrs, train, val, xlabel, ylabel, plot_title):
    if len(history_arrs) == 1:
        history = history_arrs[0]
        plt.plot(history['accuracy'])
        plt.plot(history['val_accuracy'])
        plt.title(plot_title)
        plt.ylabel(ylabel)
        plt.xlabel(xlabel)
        plt.legend(['Train', 'Validation'], loc='upper left')
        plt.show()
    else:
        f, a = plt.subplots(1, len(history_arrs), figsize=(10,5))
        for idx, history in enumerate(history_arrs):
            # For Sine Function
            a[idx].plot(history[train])
            a[idx].plot(history[val])
            title = plot_title + ' ' + str(idx)
            a[idx].set_title(title)
            a[idx].set_xlabel(xlabel)
            a[idx].set_ylabel(ylabel)
            a[idx].legend(['Train', 'Validation'], loc='upper left')
        f.tight_layout()
        plt.show()

def calculate_metrics(model, X_test, y_test):
    ypred_class = model.predict_classes(X_test, verbose=0)
    ypred_class = ypred_class[:, 0]
    accuracy = accuracy_score(y_test, ypred_class)
    precision = precision_score(y_test, ypred_class)
    recall = recall_score(y_test, ypred_class)
    f1 = f1_score(y_test, ypred_class)
    conf_matrix = confusion_matrix(y_test, ypred_class)
    return accuracy, precision, recall, f1, conf_matrix

def print_conf_matrix(conf_matrix):
    cm = pd.DataFrame(
    conf_matrix,
    index=['true:positive', 'true:negative'],
    columns=['pred:positive', 'pred:negative']
    )
    print(cm)

def model_lstm(embedding_layer, lr):
    model = Sequential()
    model.add(embedding_layer)
    model.add(LSTM(128))
    model.add(Dense(32))
    model.add(Dense(1, activation='sigmoid'))
    # Define Adam optimizer with specified learning rate
    adam_optimizer = Adam(learning_rate=lr)
    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])
    return model

def model_bilstm(embedding_layer, lr):
    model = Sequential()
    model.add(embedding_layer)
    model.add(Bidirectional(LSTM(128)))
    model.add(Dense(1, activation='sigmoid'))
    # Define Adam optimizer with specified learning rate
    adam_optimizer = Adam(learning_rate=lr)
    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])
    return model

def model_cnnlstm(embedding_layer, lr):
    model = Sequential()
    model.add(embedding_layer)
    model.add(Conv1D(128, 2, activation='relu'))
    model.add(Conv1D(64, 2, activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(LSTM(64))
    model.add(Dense(1, activation='sigmoid'))
    # Define Adam optimizer with specified learning rate
    adam_optimizer = Adam(learning_rate=lr)
    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])
    return model

def model_cnnbilstm(embedding_layer, lr):
    model = Sequential()
    model.add(embedding_layer)
    model.add(Conv1D(128, 2, activation='relu'))
    model.add(Conv1D(64, 2, activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Bidirectional(LSTM(128)))
    model.add(Dense(1, activation='sigmoid'))
    # Define Adam optimizer with specified learning rate
    adam_optimizer = Adam(learning_rate=lr)
    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])
    return model

#Gridsearch parameters
epochs = 20
lr = [0.1, 0.01, 0.001, 0.0001]
batch = [16, 32, 64]

"""Gridsearch for LSTM model"""

best_result = []
best_acc = 0
for lrate in lr:
    for b in batch:
        embedding = Embedding(input_dim, output_dim, trainable = True)
        model = model_lstm(embedding, lrate)
        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])
        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)
        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)
        if acc > best_acc:
            best_acc = acc
            best_result = [lrate, b]
print('Best performing paramters:')
print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))

"""Gridsearch for BiLSTM model"""

best_result = []
best_acc = 0
for lrate in lr:
    for b in batch:
        embedding = Embedding(input_dim, output_dim, trainable = True)
        model = model_bilstm(embedding, lrate)
        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])
        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)
        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)
        if acc > best_acc:
            best_acc = acc
            best_result = [lrate, b]
print('Best performing paramters:')
print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))

"""
Gridsearch for CNN-LSTM model"""

best_result = []
best_acc = 0
for lrate in lr:
    for b in batch:
        embedding = Embedding(input_dim, output_dim, trainable = True)
        model = model_cnnlstm(embedding, lrate)
        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])
        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)
        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)
        if acc > best_acc:
            best_acc = acc
            best_result = [lrate, b]
print('Best performing paramters:')
print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))

"""Gridsearch for CNN-BiLSTM model"""

best_result = []
best_acc = 0
for lrate in lr:
    for b in batch:
        embedding = Embedding(input_dim, output_dim, trainable = True)
        model = model_cnnbilstm(embedding, lrate)
        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])
        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)
        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)
        if acc > best_acc:
            best_acc = acc
            best_result = [lrate, b]
print('Best performing paramters:')
print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))