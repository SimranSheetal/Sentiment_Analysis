# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15I0vZoiGd5bIHMsq4RrUcl3KeOuXXV9a

Get my own dataset
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
cmt_pos = pd.read_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_pos.csv')
cmt_neg = pd.read_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_neg.csv')

negative_indices = cmt_neg.index.tolist()
diff = abs(len(cmt_neg) - len(cmt_pos))
indices = np.random.choice(negative_indices, diff, replace=False)
cmt_neg = cmt_neg.drop(indices)
df = pd.concat([cmt_pos, cmt_neg], ignore_index=True)
X = df.body
Y = df.rating.values

from sklearn.model_selection import train_test_split

seed = 42

# Split data into train and test sets (80% train, 20% test)
train, test = train_test_split(df, random_state=seed, test_size=0.2, stratify=df.rating)

# Further split the train set into train and validation sets (87.5% train, 12.5% validation)
train, val = train_test_split(train, random_state=seed, test_size=0.125, stratify=train.rating)

# Print the size of each set
print('Train set size:', train.shape)  # Corrected this line to make it valid Python
print('Test set size:', test.shape)
print('Validation set size:', val.shape)

train.to_csv('/content/drive/MyDrive/myproject/datasets/split/train.csv', index=False)
test.to_csv('/content/drive/MyDrive/myproject/datasets/split/test.csv', index=False)
val.to_csv('/content/drive/MyDrive/myproject/datasets/split/val.csv', index=False)