# -*- coding: utf-8 -*-
"""DATAPreProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wmmdjb5A8bTkXe3a1ycdssHYAV4N1OSg
"""

# Step 1: Install the required libraries
!apt-get install -y libenchant-2-2
!pip install pandas numpy nltk pyenchant

# Step 2: Import the libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
import enchant
from enchant.checker import SpellChecker

# Optional: Download NLTK data files
nltk.download('stopwords')
nltk.download('punkt')

import sys
print("Python version")
print (sys.version)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
path = '/content/drive/MyDrive/myproject/datasets/labelled/'
print(os.listdir(path))

df = pd.read_csv(r'/content/drive/MyDrive/myproject/datasets/labelled/unpreprocessed.csv')

"""Total Number of Data"""

df.shape[0]

"""MAKING OF DATA **UNIFORM**

Remove url
"""

df['preproc'] = df.body.replace(r'http\S+', '', regex=True)

from google.colab import files
uploaded = files.upload()

!pip install emoji demoji

from google.colab import files
uploaded = files.upload()  # This will prompt you to upload your file

from acronyms_smileys import smileys, sent_acronyms

import os
print(os.getcwd())  # This will show you the current working directory

os.listdir()  # This will list all files in the current working directory

from acronyms_smileys import smileys
from acronyms_smileys import sent_acronyms

def replace_repeating_emoji(text):
    uniques = set()
    final_string = list()
    text_arr = [item for item in emoji.get_emoji_regexp().split(text) if not item == '']
    for e in text_arr:
        # for some reason even though it is defined as '❤', when its
        # imported, it gets loaded as '❤❤'
        if e == '❤':
            e = '❤❤'
        if not bool(emoji.get_emoji_regexp().search(e)):
            final_string.append(smileys.get(e, e))
        else:
            if e not in uniques:
                uniques.add(e)
                final_string.append(smileys.get(e, e))
    return ' '.join(final_string)

import re

def replace_repeating_emoji(text):
    # Regex pattern for detecting emojis (basic version)
    emoji_pattern = r'([\U0001F600-\U0001F64F]|[\U0001F300-\U0001F5FF]|[\U0001F680-\U0001F6FF]|[\U0001F700-\U0001F77F]|[\U0001F780-\U0001F8FF]|[\U00002702-\U000027B0]|[\U000024C2-\U0001F251])'

    # Split the text into parts (emojis and non-emojis)
    text_arr = re.split(f'({emoji_pattern})', text)
    final_string = []

    for e in text_arr:
        # If it is an emoji, check for repetition
        if e and re.search(emoji_pattern, e):
            # Check for repetition of the same emoji
            if len(e) > 1 and e[0] == e[1]:
                final_string.append(e[0])  # Keep only one instance of the repeating emoji
            else:
                final_string.append(e)  # Keep as is
        else:
            final_string.append(e)  # Non-emoji parts stay unchanged

    return ''.join(final_string)

"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')

!pip install --upgrade emoji

import emoji
import demoji

# remove untagged emoticons
df.preproc = df.preproc.apply(lambda x : demoji.replace(x, ''))

"""Set sentiment on acronyms (such as 'lol')"""

df.preproc = df.preproc.apply(lambda x: ' '.join(sent_acronyms.get(word, word) for word in x.split()))

"""Remove hashtags"""

df.preproc = df.preproc.apply(lambda x: ' '.join([word for word in x.split() if '#' not in word]))
df.loc[df.preproc.str.contains('#')]

"""Replace negations with "not"
"""

negations = ['don\'t', 'aint' 'aren\'t', 'couldn\'t','didn\'t',
             'doesn\'t', 'hadn\'t', 'hasn\'t', 'haven\'t', 'isn\'t',
             'mightn\'t', 'mustn\'t', 'needn\'t', 'shouldn\'t', 'wasn\'t',
             'weren\'t', 'won\'t', 'wouldn\'t', 'nor', 'not', 'cant', 'dont',
            'arent', 'couldnt', 'didnt', 'doesnt', 'hadnt', 'hasnt', 'havent',
            'isnt', 'mightnt', 'mustnt', 'neednt', 'shouldnt', 'wasnt',
            'werent', 'wont', 'wouldnt']
regx = r'\b(?:{})\b'.format('|'.join(negations))
df.preproc = df.preproc.str.replace(regx, 'not')

# Remove negations from stop list, add two missing contractions
stopwords_list = stopwords.words('english')
stopwords_list = [el for el in stopwords_list if el not in negations]
missing_words = ['i\'m', 'i\'d']
stopwords_list.extend(missing_words)

singular_pronouns = ['me', 'you', 'he', 'she', 'they', 'his', 'her', 'him']
stopwords_list_complete = [x for x in stopwords_list if x not in singular_pronouns]

"""20 most commonly used words"""

pd.Series(' '.join(df.preproc).lower().split()).value_counts()[:20]

def get_common_stopwords(stop_words, n=5):
    most_freq_words = pd.Series(' '.join(df.preproc).lower().split()).value_counts()[:int(n*2)].keys().to_numpy()
    common_stopwords = [i for i in most_freq_words if i in stopwords_list_complete]
    return common_stopwords[0:n]

"""Remove most common stop words"""

common_stopwords = get_common_stopwords(stopwords_list, n=20)
df.preproc = df.preproc.apply(lambda x: ' '.join([word for word in x.split() if word not in (common_stopwords)]))

df.head(5)

"""Remove punctuation"""

import string
df.preproc = df.preproc.str.replace('[{}]'.format(string.punctuation), '')

df.head(5)

"""Remove repeating vowels and consonants"""

df.loc[df.positive == 1].head(10)

# https://stackoverflow.com/questions/46701245/how-to-replace-multiple-consecutive-repeating-characters-into-1-character-in-pyt
df.preproc = df.preproc.apply(lambda x: ' '.join([re.sub(r'(.)\1{2,}', r'\1\1', word) for word in x.split()]))

"""Tag any sequence of "ha" or "ah" (for example, "ahaha" or "haha") as a "laugh"
"""

df.preproc = df.preproc.apply(lambda x: ' '.join([re.sub(r'([ha]+[ah]+).*\1', r'laugh', word) for word in x.split()]))

"""Remove numbers"""

def assign_rating(row):
    if row['positive'] == 1:
        return 1
    elif row['negative'] == 1:
        return 0
    else:
        return -1

# 0 - negative, 1 - positive, -1 neutral
df['rating'] = df.apply(lambda row: assign_rating(row), axis=1)

preprocessed = df.filter(['comment_id', 'video_id', 'date', 'preproc', 'rating'], axis=1)
preprocessed.columns = ['comment_id', 'video_id', 'date', 'body', 'rating']

preprocessed = df.loc[df.body != '']

preprocessed = preprocessed.loc[preprocessed.body != '']



preprocessed.to_csv('/content/drive/MyDrive/myproject/datasets/processed/processed.csv', index=False)
preprocessed.loc[preprocessed.rating == 1].to_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_pos.csv', index=False)
preprocessed.loc[preprocessed.rating == 0].to_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_neg.csv', index=False)

preprocessed.loc[preprocessed.rating == 1].to_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_pos.csv', index=False)
preprocessed.loc[preprocessed.rating == 1].to_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_neg.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np

# Load the processed datasets from Google Drive
cmt_pos = pd.read_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_pos.csv')
cmt_neg = pd.read_csv('/content/drive/MyDrive/myproject/datasets/processed/processed_neg.csv')